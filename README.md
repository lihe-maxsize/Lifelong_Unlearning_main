# LLaVA Unlearning
This codebase is developed based on the LLaVA source code. 
To use it, please install all the environment dependencies required by LLaVA.

# Unlearning
The code for baselines unlearning is in unlearning_auto.py.

# Eval
The code for model evaluation (get model response) is in eval_unlearned_model.py.

# Metrics
The code for automatic evaluation of model responses is in metrics.py.

# Gate Module
The code for automatic entities extraction of GLM and task match is in entity_extract.py.

# LUMoE
The code for LUMoE method is in LUMoE.py.

# Benchmark
The text benchmark data (QA pairs) is in file folder of tasks.
Due to the size limit, we can not release the image data in the supplementary material and this repository.
The full MLUbench is avaliable at https://1drv.ms/f/c/419e3e9d51ec307d/Ej7v9Yo_Q6BOsb5z2nzy1w8BMIvrEZUZG0fYg7DM2p-8ig 

